C1：
Human-agent engineering 
Focus on the skills that are not yet replaced by AI systems
Business understanding

Become the tech lead
LLMs are only as good as you are
Good context leads to good code
If you can’t understand your codebase, neither will an LLM

Read and review a lot of code
Learn to discern good from bad, wrong software
Have good taste

Experiment aggressively
There are no established software patterns yet
Everyone is still figuring it out
This class will introduce many workflows and tools - figure out what works for you


how llm being traning 
Stage 1
Self-supervised pretraining
Teach the model notion of language on a variety of often public data sources
100s of billions to trillion+ tokens (language and code)
Common Crawl, Wikipedia, StackExchange, Public Github repos
Write a for loop → that could be used in a piece of code
Stage 2
Supervised finetuning
Teach model to follow instructions
High-quality, curated prompt-response pairs (“what is the capital of Croatia” -> “Zagreb is the capital”)
Tens of thousands to 100s of thousands of pairs 
Write a for loop → ok here’s a for loop…
Stage 3
Preferencing tuning
Align model outputs with human preferences (helpfulness, correctness, readability)
Collect pairs of outputs for same prompt and train reward model to predict preferred output
Tens of thousands to 100s of thousands of human-labeled comparisons
Write a for loop → for idx in range(10):



Reasoning models
Extend training with chain-of-thought reasoning traces
Tool-use integration
Get human preferences on reasoning steps
Reinforcement learning to learn how to evaluate reasoning traces, backtrack, etc
Size
GPT-3/Claude 3.5 Sonnet - 175B parameters
LLaMA 3.1 - 405B parameters
GPT-4 - 1.8T (reported)

Prompts should be formatted with structure
Here are the logs:
<log>LOG MESSAGE<log>
and the stack trace:
<error>STACK TRACE<error>

Best Practices
Be explicit about what you want (languages, tech stacks, libraries, constraints)
Decompose tasks

